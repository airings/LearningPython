{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DataFrames from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NOC         Country   Total\n",
      "0  USA   United States  2088.0\n",
      "1  URS    Soviet Union   838.0\n",
      "2  GBR  United Kingdom   498.0\n",
      "3  FRA          France   378.0\n",
      "4  GER         Germany   407.0\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'Bronze.csv' into a DataFrame: bronze\n",
    "bronze = pd.read_csv('Bronze.csv')\n",
    "\n",
    "# Read 'Silver.csv' into a DataFrame: silver\n",
    "silver = pd.read_csv('Silver.csv')\n",
    "\n",
    "# Read 'Gold.csv' into a DataFrame: gold\n",
    "gold = pd.read_csv('Gold.csv')\n",
    "\n",
    "# Print the first five rows of gold\n",
    "print(gold.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NOC         Country   Total\n",
      "0  USA   United States  2088.0\n",
      "1  URS    Soviet Union   838.0\n",
      "2  GBR  United Kingdom   498.0\n",
      "3  FRA          France   378.0\n",
      "4  GER         Germany   407.0\n"
     ]
    }
   ],
   "source": [
    "# loading data from multiple files into DataFrames is more efficient in a loop or a comprehension.\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create the list of file names: filenames\n",
    "filenames = ['Gold.csv', 'Silver.csv', 'Bronze.csv']\n",
    "\n",
    "# Make list of three DataFrames loaded from filenames: dataframes\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "# Print top 5 rows of 1st DataFrame in dataframes\n",
    "print(dataframes[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NOC         Country    Gold  Silver  Bronze\n",
      "0  USA   United States  2088.0  1195.0  1052.0\n",
      "1  URS    Soviet Union   838.0   627.0   584.0\n",
      "2  GBR  United Kingdom   498.0   591.0   505.0\n",
      "3  FRA          France   378.0   461.0   475.0\n",
      "4  GER         Germany   407.0   350.0   454.0\n"
     ]
    }
   ],
   "source": [
    "# Combining DataFrames from multiple data files\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Make a copy of gold: medals\n",
    "medals = gold.copy()\n",
    "\n",
    "# Create list of new column labels: new_labels\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "\n",
    "# Rename the columns of medals using new_labels\n",
    "medals.columns = new_labels\n",
    "\n",
    "# Add columns 'Silver' & 'Bronze' to medals\n",
    "medals['Silver'] = silver['Total']\n",
    "medals['Bronze'] = bronze['Total']\n",
    "\n",
    "# Print the head of medals\n",
    "print(medals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting DataFrame with the Index & columns\n",
    "\n",
    "It is often useful to rearrange the sequence of the rows of a DataFrame by sorting. You don't have to implement these yourself; the principal methods for doing this are .sort_index() and .sort_values()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Max TemperatureF\n",
      "Month                  \n",
      "Jan                  68\n",
      "Feb                  60\n",
      "Mar                  68\n",
      "Apr                  84\n",
      "May                  88\n",
      "       Max TemperatureF\n",
      "Month                  \n",
      "Apr                  84\n",
      "Aug                  86\n",
      "Dec                  68\n",
      "Feb                  60\n",
      "Jan                  68\n",
      "       Max TemperatureF\n",
      "Month                  \n",
      "Sep                  90\n",
      "Oct                  84\n",
      "Nov                  72\n",
      "May                  88\n",
      "Mar                  68\n",
      "       Max TemperatureF\n",
      "Month                  \n",
      "Feb                  60\n",
      "Jan                  68\n",
      "Mar                  68\n",
      "Dec                  68\n",
      "Nov                  72\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
    "weather1 = pd.read_csv('monthly_max_temp.csv', index_col='Month')\n",
    "\n",
    "# Print the head of weather1\n",
    "print(weather1.head())\n",
    "\n",
    "# Sort the index of weather1 in alphabetical order: weather2\n",
    "weather2 = weather1.sort_index()\n",
    "\n",
    "# Print the head of weather2\n",
    "print(weather2.head())\n",
    "\n",
    "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "\n",
    "# Print the head of weather3\n",
    "print(weather3.head())\n",
    "\n",
    "# Sort the index of weather1 numerically using the values of 'Max TemperatureF': weather4\n",
    "weather4 = weather1.sort_values('Max TemperatureF')\n",
    "\n",
    "# Print the head of weather4\n",
    "print(weather4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Max TemperatureF\n",
      "Month                  \n",
      "Jan                  68\n",
      "Feb                  60\n",
      "Mar                  68\n",
      "Apr                  84\n",
      "May                  88\n",
      "Jun                  89\n",
      "Jul                  91\n",
      "Aug                  86\n",
      "Sep                  90\n",
      "Oct                  84\n",
      "Nov                  72\n",
      "Dec                  68\n",
      "       Max TemperatureF\n",
      "Month                  \n",
      "Jan                  68\n",
      "Feb                  60\n",
      "Mar                  68\n",
      "Apr                  84\n",
      "May                  88\n",
      "Jun                  89\n",
      "Jul                  91\n",
      "Aug                  86\n",
      "Sep                  90\n",
      "Oct                  84\n",
      "Nov                  72\n",
      "Dec                  68\n"
     ]
    }
   ],
   "source": [
    "# Sorting methods are not the only way to change DataFrame Indexes. There is also the .reindex() method.\n",
    "\n",
    "# weather1\n",
    "#        Mean TemperatureF\n",
    "# Month                   \n",
    "# Apr            61.956044\n",
    "# Jan            32.133333\n",
    "# Jul            68.934783\n",
    "# Oct            43.434783\n",
    "\n",
    "# Notice that values corresponding to months missing from weather1\n",
    "# are filled with NaN values in weather2. This does not happen in weather3, since you used forward-fill.\n",
    "\n",
    "import pandas as pd\n",
    "year = ['Jan',\n",
    "         'Feb',\n",
    "         'Mar',\n",
    "         'Apr',\n",
    "         'May',\n",
    "         'Jun',\n",
    "         'Jul',\n",
    "         'Aug',\n",
    "         'Sep',\n",
    "         'Oct',\n",
    "         'Nov',\n",
    "         'Dec']\n",
    "# Reindex weather1 using the list year: weather2\n",
    "weather2 = weather1.reindex(year)\n",
    "\n",
    "# Print weather2: has nan\n",
    "print(weather2)\n",
    "\n",
    "# Reindex weather1 using the list year with forward-fill: weather3\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "# Print weather3\n",
    "print(weather3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindexing using another DataFrame Index\n",
    "Another common technique is to reindex a DataFrame using the Index of another DataFrame. The DataFrame .reindex() method can accept the Index of a DataFrame or Series as input. You can access the Index of a DataFrame with its .index attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Reindex names_1981 with index of names_1881: common_names\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "\n",
    "# Print shape of common_names\n",
    "print(common_names.shape)\n",
    "\n",
    "# Drop rows with null counts: common_names\n",
    "common_names = common_names.dropna()\n",
    "\n",
    "# Print shape of new common_names\n",
    "print(common_names.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# january\n",
    "#                   Units\n",
    "# Company                \n",
    "# Acme Corporation     19\n",
    "# Hooli                17\n",
    "# Initech              20\n",
    "# Mediacore            10\n",
    "# Streeplex            13\n",
    "\n",
    "# february\n",
    "#                   Units\n",
    "# Company                \n",
    "# Acme Corporation     15\n",
    "# Hooli                 3\n",
    "# Mediacore            13\n",
    "# Vandelay Inc         25\n",
    "\n",
    "# In [1]: january + february\n",
    "# Out[1]: \n",
    "#                   Units\n",
    "# Company                \n",
    "# Acme Corporation   34.0\n",
    "# Hooli              20.0\n",
    "# Initech             NaN\n",
    "# Mediacore          23.0\n",
    "# Streeplex           NaN\n",
    "# Vandelay Inc        NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting in arithmetic formulas\n",
    "In this exercise, you'll work with weather data pulled from wunderground.com. The DataFrame weather has been pre-loaded along with pandas as pd. It has 365 rows (observed each day of the year 2013 in Pittsburgh, PA) and 22 columns reflecting different weather measurements each day.\n",
    "\n",
    "You'll subset a collection of columns related to temperature measurements in degrees Fahrenheit, convert them to degrees Celsius, and relabel the columns of the new DataFrame to reflect the change of units.\n",
    "\n",
    "Remember, ordinary arithmetic operators (like +, -, *, and /) broadcast scalar values to conforming DataFrames when combining scalars & DataFrames in arithmetic expressions. Broadcasting also works with pandas Series and NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract selected columns from weather as new DataFrame: temps_f\n",
    "temps_f = weather[['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n",
    "\n",
    "# Convert temps_f to celsius: temps_c\n",
    "temps_c = (temps_f -32) * 5/9\n",
    "\n",
    "# Rename 'F' in column names with 'C': temps_c.columns\n",
    "temps_c.columns = temps_c.columns.str.replace('F', 'C')\n",
    "\n",
    "# Print first 5 rows of temps_c\n",
    "print(temps_c.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VALUE\n",
      "DATE               \n",
      "2014-07-01  17569.4\n",
      "2014-10-01  17692.2\n",
      "2015-01-01  17783.6\n",
      "2015-04-01  17998.3\n",
      "2015-07-01  18141.9\n",
      "2015-10-01  18222.8\n",
      "2016-01-01  18281.6\n",
      "2016-04-01  18436.5\n",
      "              VALUE\n",
      "DATE               \n",
      "2008-12-31  14549.9\n",
      "2009-12-31  14566.5\n",
      "2010-12-31  15230.2\n",
      "2011-12-31  15785.3\n",
      "2012-12-31  16297.3\n",
      "2013-12-31  16999.9\n",
      "2014-12-31  17692.2\n",
      "2015-12-31  18222.8\n",
      "2016-12-31  18436.5\n",
      "              VALUE    growth\n",
      "DATE                         \n",
      "2008-12-31  14549.9       NaN\n",
      "2009-12-31  14566.5  0.114090\n",
      "2010-12-31  15230.2  4.556345\n",
      "2011-12-31  15785.3  3.644732\n",
      "2012-12-31  16297.3  3.243524\n",
      "2013-12-31  16999.9  4.311144\n",
      "2014-12-31  17692.2  4.072377\n",
      "2015-12-31  18222.8  2.999062\n",
      "2016-12-31  18436.5  1.172707\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read 'GDP.csv' into a DataFrame: gdp\n",
    "gdp = pd.read_csv('GDP.csv', index_col='DATE', parse_dates=True)\n",
    "\n",
    "# Slice all the gdp data from 2008 onward: post2008\n",
    "post2008 = gdp['2008':]\n",
    "\n",
    "# Print the last 8 rows of post2008\n",
    "print(post2008.tail(8))\n",
    "\n",
    "# Resample post2008 by year, keeping last(): yearly\n",
    "yearly = post2008.resample('A').last()\n",
    "\n",
    "# Print yearly\n",
    "print(yearly)\n",
    "\n",
    "# Compute percentage growth of yearly: yearly['growth']\n",
    "yearly['growth'] = yearly.pct_change() * 100\n",
    "\n",
    "# Print yearly again\n",
    "print(yearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'sp500.csv' into a DataFrame: sp500\n",
    "sp500 = pd.read_csv('sp500.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Read 'exchange.csv' into a DataFrame: exchange\n",
    "exchange = pd.read_csv('exchange.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
    "dollars = sp500[['Open', 'Close']]\n",
    "\n",
    "# Print the head of dollars\n",
    "print(dollars.head())\n",
    "\n",
    "# Convert dollars to pounds: pounds\n",
    "pounds = dollars.multiply(exchange['GBP/USD'], axis='rows')\n",
    "\n",
    "# Print the head of pounds\n",
    "print(pounds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv('sales-jan-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv('sales-feb-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv('sales-mar-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating pandas Series along row axis\n",
    "You may be wondering about the difference between pd.concat() and pandas' .append() method. One way to think of the difference is that .append() is a specific case of a concatenation, while pd.concat()Concatenating pandas Series along row axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month['Units'])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units, axis='rows')\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names.loc[combined_names.name == 'Morgan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate weather_max and weather_mean horizontally: weather\n",
    "weather = pd.concat([weather_max, weather_mean], axis=1)\n",
    "\n",
    "# Print weather\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple files to build a DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the keys parameter in the call to pd.concat(), which generates a hierarchical index with the labels from keys as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for medal in medal_types:\n",
    "\n",
    "    # Create the file name: file_name\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    \n",
    "    # Create list of column names: columns\n",
    "    columns = ['Country', medal]\n",
    "    \n",
    "    # Read file_name into a DataFrame: df\n",
    "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
    "\n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "# Concatenate medals horizontally: medals\n",
    "medals = pd.concat(medals, axis='columns')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing MultiIndexed DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the entries of medals: medals_sorted\n",
    "# how to sort ? \n",
    "# what is level 0 or level 1?\n",
    "medals_sorted = medals.sort_index(level=0)\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:, 'United Kingdom'], :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, axis=1, keys=['Hardware', 'Software', 'Service'])\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['Feb. 2, 2015':'Feb. 8, 2015', idx[:, 'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating DataFrames from a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = dict(month_list)\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, axis=1, keys=['bronze', 'silver', 'gold'], join='inner')\n",
    "\n",
    "# Print medals\n",
    "print(medals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the method .resample() must be chained with some kind of \n",
    "# aggregation method (.pct_change() and .last() in this case).\n",
    "\n",
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').pct_change(10).dropna()\n",
    "\n",
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').pct_change(10).dropna()\n",
    "\n",
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual,us_annual], join='inner', axis=1)\n",
    "\n",
    "# Resample gdp and print\n",
    "print(gdp.resample('10A').last())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge revenue with managers on 'city': merge_by_city\n",
    "merge_by_city = pd.merge(revenue, managers, on='city')\n",
    "\n",
    "# Print merge_by_city\n",
    "print(merge_by_city)\n",
    "\n",
    "# Merge revenue with managers on 'branch_id': merge_by_id\n",
    "merge_by_id = pd.merge(revenue, managers, on='branch_id')\n",
    "\n",
    "# Print merge_by_id\n",
    "print(merge_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge revenue & managers on 'city' & 'branch': combined\n",
    "combined = pd.merge(revenue, managers, left_on='city', right_on='branch')\n",
    "\n",
    "# Print combined\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add 'state' column to revenue: revenue['state']\n",
    "revenue['state'] = ['TX','CO','IL','CA']\n",
    "\n",
    "# Add 'state' column to managers: managers['state']\n",
    "managers['state'] = ['TX','CO','CA','MO']\n",
    "\n",
    "# Merge revenue & managers on 'branch_id', 'city', & 'state': combined\n",
    "combined = pd.merge(revenue,managers, on=['branch_id', 'city', 'state'])\n",
    "\n",
    "# Print combined\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge revenue and sales: revenue_and_sales\n",
    "revenue_and_sales = pd.merge(revenue, sales, how='right', on=['city', 'state'])\n",
    "\n",
    "# Print revenue_and_sales\n",
    "print(revenue_and_sales)\n",
    "\n",
    "# Merge sales and managers: sales_and_managers\n",
    "sales_and_managers = pd.merge(sales, managers, how='left', left_on=['city', 'state'], right_on=['branch', 'state'])\n",
    "\n",
    "# Print sales_and_managers\n",
    "print(sales_and_managers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the first merge: merge_default\n",
    "merge_default = pd.merge(sales_and_managers, revenue_and_sales)\n",
    "\n",
    "# Print merge_default\n",
    "print(merge_default)\n",
    "\n",
    "# Perform the second merge: merge_outer\n",
    "merge_outer = pd.merge(sales_and_managers, revenue_and_sales, how='outer')\n",
    "\n",
    "# Print merge_outer\n",
    "print(merge_outer)\n",
    "\n",
    "# Perform the third merge: merge_outer_on\n",
    "merge_outer_on = pd.merge(sales_and_managers, revenue_and_sales, how='outer', on=['city','state'])\n",
    "\n",
    "# Print merge_outer_on\n",
    "print(merge_outer_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using merge_ordered()\n",
    "This exercise uses pre-loaded DataFrames austin and houston that contain weather data from the cities Austin and Houston respectively. They have been printed in the IPython Shell for you to examine.\n",
    "\n",
    "Weather conditions were recorded on separate days and you need to merge these two DataFrames together such that the dates are ordered. To do this, you'll use pd.merge_ordered(). After you're done, note the order of the rows before and after merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the first ordered merge: tx_weather\n",
    "tx_weather = pd.merge_ordered(austin, houston)\n",
    "\n",
    "# Print tx_weather\n",
    "print(tx_weather)\n",
    "\n",
    "# Perform the second ordered merge: tx_weather_suff\n",
    "tx_weather_suff = pd.merge_ordered(austin,houston,on='date',suffixes=['_aus','_hus'])\n",
    "\n",
    "# Print tx_weather_suff\n",
    "print(tx_weather_suff)\n",
    "\n",
    "# Perform the third ordered merge: tx_weather_ffill\n",
    "tx_weather_ffill = pd.merge_ordered(austin,houston,on='date',suffixes=['_aus','_hus'], fill_method='ffill')\n",
    "\n",
    "# Print tx_weather_ffill\n",
    "print(tx_weather_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using merge_asof()\n",
    "Similar to pd.merge_ordered(), the pd.merge_asof() function will also merge values in order using the on column, but for each row in the left DataFrame, only rows from the right DataFrame whose 'on' column values are less than the left value will be kept.\n",
    "\n",
    "This function can be use to align disparate datetime frequencies without having to first resample.\n",
    "\n",
    "Here, you'll merge monthly oil prices (US dollars) into a full automobile fuel efficiency dataset. The oil and automobile DataFrames have been pre-loaded as oil and auto. The first 5 rows of each have been printed in the IPython Shell for you to explore.\n",
    "\n",
    "These datasets will align such that first price of year will be broadcast into the rows of the automobiles DataFrame. This is considered correct since by the start of any given year, most automobiles for that year will have already been manufactured.\n",
    "\n",
    "You'll then inspect the merged DataFrame, resample by year and compute the mean 'Price' and 'mpg'. You should be able to see a trend in these two columns, that you can confirm by computing the Pearson correlation between resampled 'Price' and 'mpg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge automobile and oil: merged\n",
    "merged = pd.merge_asof(auto,oil,left_on='yr',right_on='Date')\n",
    "\n",
    "# Print the tail of merged\n",
    "print(merged.tail())\n",
    "\n",
    "# Resample merged: yearly\n",
    "yearly = merged.resample('A', on='Date')[['mpg','Price']].mean()\n",
    "\n",
    "# Print yearly\n",
    "print(yearly)\n",
    "\n",
    "# print yearly.corr()\n",
    "print(yearly.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - Summer Olympics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create file path: file_path\n",
    "file_path = 'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'\n",
    "\n",
    "# Load DataFrame from file_path: editions\n",
    "editions = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Extract the relevant columns: editions\n",
    "editions = editions[['Edition', 'Grand Total', 'City', 'Country']]\n",
    "\n",
    "# Print editions DataFrame\n",
    "print(editions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create the file path: file_path\n",
    "file_path = 'Summer Olympic medallists 1896 to 2008 - IOC COUNTRY CODES.csv'\n",
    "\n",
    "# Load DataFrame from file_path: ioc_codes\n",
    "ioc_codes = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the relevant columns: ioc_codes\n",
    "ioc_codes = ioc_codes[['Country', 'NOC']]\n",
    "\n",
    "# Print first and last 5 rows of ioc_codes\n",
    "print(ioc_codes.head())\n",
    "print(ioc_codes.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty dictionary: medals_dict\n",
    "medals_dict = {}\n",
    "\n",
    "for year in editions['Edition']:\n",
    "\n",
    "    # Create the file path: file_path\n",
    "    file_path = 'summer_{:d}.csv'.format(year)\n",
    "    \n",
    "    # Load file_path into a DataFrame: medals_dict[year]\n",
    "    medals_dict[year] = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns: medals_dict[year]\n",
    "    medals_dict[year] = medals_dict[year][['Athlete', 'NOC', 'Medal']]\n",
    "    \n",
    "    # Assign year to column 'Edition' of medals_dict\n",
    "    medals_dict[year]['Edition'] = year\n",
    "    \n",
    "# Concatenate medals_dict: medals\n",
    "medals = pd.concat(medals_dict, ignore_index=True)\n",
    "\n",
    "# Print first and last 5 rows of medals\n",
    "print(medals.head())\n",
    "print(medals.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the pivot_table: medal_counts\n",
    "medal_counts = medals.pivot_table(index='Edition', values='Athlete', columns='NOC', aggfunc='count')\n",
    "\n",
    "# Print the first & last 5 rows of medal_counts\n",
    "print(medal_counts.head())\n",
    "print(medal_counts.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Index of editions: totals\n",
    "totals = editions.set_index('Edition')\n",
    "\n",
    "# Reassign totals['Grand Total']: totals\n",
    "totals = totals['Grand Total']\n",
    "\n",
    "# Divide medal_counts by totals: fractions\n",
    "fractions = medal_counts.divide(totals, axis='rows')\n",
    "\n",
    "# Print first & last 5 rows of fractions\n",
    "print(fractions.head())\n",
    "print(fractions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the expanding mean: mean_fractions\n",
    "mean_fractions = fractions.expanding().mean()\n",
    "\n",
    "# Compute the percentage change: fractions_change\n",
    "fractions_change = mean_fractions.pct_change() * 100\n",
    "\n",
    "# Reset the index of fractions_change: fractions_change\n",
    "fractions_change = fractions_change.reset_index()\n",
    "\n",
    "# Print first & last 5 rows of fractions_change\n",
    "print(fractions_change.head())\n",
    "print(fractions_change.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Left join editions and ioc_codes: hosts\n",
    "hosts = pd.merge(editions, ioc_codes, how='left')\n",
    "\n",
    "# Extract relevant columns and set index: hosts\n",
    "hosts = hosts[['Edition', 'NOC']].set_index('Edition')\n",
    "\n",
    "# Fix missing 'NOC' values of hosts\n",
    "print(hosts.loc[hosts.NOC.isnull()])\n",
    "hosts.loc[1972, 'NOC'] = 'FRG'\n",
    "hosts.loc[1980, 'NOC'] = 'URS'\n",
    "hosts.loc[1988, 'NOC'] = 'KOR'\n",
    "\n",
    "# Reset Index of hosts: hosts\n",
    "hosts = hosts.reset_index()\n",
    "\n",
    "# Print hosts\n",
    "print(hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Reshape fractions_change: reshaped\n",
    "reshaped = pd.melt(fractions_change, id_vars='Edition', value_name='Change')\n",
    "\n",
    "# Print reshaped.shape and fractions_change.shape\n",
    "print(reshaped.shape, fractions_change.shape)\n",
    "\n",
    "# Extract rows from reshaped where 'NOC' == CHN': chn\n",
    "chn = reshaped[reshaped['NOC'] == 'CHN']\n",
    "\n",
    "# Print last 5 rows of chn with .tail()\n",
    "print(chn.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Merge reshaped and hosts: merged\n",
    "merged = pd.merge(reshaped,hosts,how='inner')\n",
    "\n",
    "# Print first 5 rows of merged\n",
    "print(merged.head())\n",
    "\n",
    "# Set Index of merged and sort it: influence\n",
    "influence = merged.set_index('Edition').sort_index()\n",
    "\n",
    "# Print first 5 rows of influence\n",
    "print(influence.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract influence['Change']: change\n",
    "change = influence['Change']\n",
    "\n",
    "# Make bar plot of change: ax\n",
    "ax = change.plot(kind='bar')\n",
    "\n",
    "# Customize the plot to improve readability\n",
    "ax.set_ylabel(\"% Change of Host Country Medal Count\")\n",
    "ax.set_title(\"Is there a Host Country Advantage?\")\n",
    "ax.set_xticklabels(editions['City'])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
