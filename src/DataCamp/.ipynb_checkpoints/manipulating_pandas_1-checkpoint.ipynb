{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and transforming data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              artist                  song chart weeks  test\n",
      "0            Blondie               Call Me          62    10\n",
      "1  Chistorpher Cross         Arthurs Theme          32    10\n",
      "2          Joan Jett  I Love Rock and Roll          72    10\n",
      "   year             artist                  song chart weeks\n",
      "0  1980            Blondie               Call Me          62\n",
      "1  1981  Chistorpher Cross         Arthurs Theme          32\n",
      "2  1982          Joan Jett  I Love Rock and Roll          72\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# sclicing in DataFrame is a view not a copy, but if need new line, the original are not changed\n",
    "df2 = pd.DataFrame([['1980', 'Blondie', 'Call Me', '6'],\n",
    "       ['1981', 'Chistorpher Cross', 'Arthurs Theme', '3'],\n",
    "       ['1982', 'Joan Jett', 'I Love Rock and Roll', '7']])\n",
    "df2.columns = ['year', 'artist', 'song', 'chart weeks']\n",
    "# df3 = df2.ix[:2, 'artist':'chart weeks'].copy()\n",
    "df3 = df2.ix[:2, 'artist':'chart weeks']\n",
    "df3['chart weeks'] = df3['chart weeks'] + '2'\n",
    "df3['test'] = 10\n",
    "print df3\n",
    "print df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    9\n",
       "2    9\n",
       "3    9\n",
       "4    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slicing in Series returns a view not a copy\n",
    "a = pd.Series([0,2,3,4,5])\n",
    "a[1:4] = 9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the boolean array: high_turnout\n",
    "high_turnout = election['turnout'] > 70\n",
    "\n",
    "# Filter the election DataFrame with the high_turnout array: high_turnout_df\n",
    "high_turnout_df = election[high_turnout]\n",
    "\n",
    "# Print the high_turnout_results DataFrame\n",
    "print(high_turnout_df)\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create the boolean array: too_close\n",
    "too_close = election['margin'] < 1\n",
    "\n",
    "# Assign np.nan to the 'winner' column where the results were too close to call\n",
    "election['winner'][too_close] = np.nan\n",
    "\n",
    "# Print the output of election.info()\n",
    "print(election.info())\n",
    "\n",
    "# Select the 'age' and 'cabin' columns: df\n",
    "df = titanic[['age', 'cabin']]\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Drop rows in df with how='any' and print the shape\n",
    "print(df.dropna(how='any').shape)\n",
    "\n",
    "# Drop rows in df with how='all' and print the shape\n",
    "print(df.dropna(how='all').shape)\n",
    "\n",
    "# Call .dropna() with thresh=1000 and axis='columns' and print the output of .info() from titanic\n",
    "print(titanic.dropna(thresh=1000, axis='columns').info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function to convert degrees Fahrenheit to degrees Celsius: to_celsius\n",
    "def to_celsius(F):\n",
    "    return 5/9*(F - 32)\n",
    "\n",
    "# Apply the function over 'Mean TemperatureF' and 'Mean Dew PointF': df_celsius\n",
    "df_celsius = weather[['Mean TemperatureF','Mean Dew PointF']].apply(to_celsius)\n",
    "\n",
    "# Reassign the columns df_celsius\n",
    "df_celsius.columns = ['Mean TemperatureC', 'Mean Dew PointC']\n",
    "\n",
    "# Print the output of df_celsius.head()\n",
    "print(df_celsius.head())\n",
    "\n",
    "# The .map() method is used to transform values according to a Python dictionary look-up.\n",
    "# Create the dictionary: red_vs_blue\n",
    "red_vs_blue = {'Obama':'blue' , 'Romney':'red'}\n",
    "\n",
    "# Use the dictionary to map the 'winner' column to the new column: election['color']\n",
    "election['color'] = election['winner'].map(red_vs_blue)\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())\n",
    "\n",
    "\n",
    "\n",
    "# When performance is paramount, you should avoid using .apply() and .map() \n",
    "# because those constructs perform Python for-loops over the data stored in a pandas Series or DataFrame. \n",
    "# By using vectorized functions instead, you can loop over the data at the same speed as compiled code (C, Fortran, etc.)! \n",
    "# NumPy, SciPy and pandas come with a variety of vectorized functions (called Universal Functions or UFuncs in NumPy).\n",
    "# Import zscore from scipy.stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Call zscore with election['turnout'] as input: turnout_zscore\n",
    "turnout_zscore = zscore(election['turnout'])\n",
    "\n",
    "# Print the type of turnout_zscore\n",
    "print(type(turnout_zscore))\n",
    "\n",
    "# Assign turnout_zscore to a new column: election['turnout_zscore']\n",
    "election['turnout_zscore'] = turnout_zscore\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     city weekday visitors signups\n",
      "0  Austin     Mon      326       3\n",
      "1  Austin     Sun      139       7\n",
      "2  Dallas     Mon      456       5\n",
      "3  Dallas     Sun      237      12\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'upper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-258c90303fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Create the list of new indexes: new_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mnew_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Assign new_idx to sales.index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'upper'"
     ]
    }
   ],
   "source": [
    "# indexes are immutable objects. This means that if you want to change or modify the index in a dataframe, \n",
    "# then you need to change the whole index.\n",
    "\n",
    "# In [1]: sales\n",
    "# Out[1]: \n",
    "#              eggs  salt  spam\n",
    "# state month                  \n",
    "# CA    1        47  12.0    17\n",
    "#       2       110  50.0    31\n",
    "# NY    1       221  89.0    72\n",
    "#       2        77  87.0    20\n",
    "# TX    1       132   NaN    52\n",
    "#       2       205  60.0    55\n",
    "\n",
    "# Create the list of new indexes: new_idx\n",
    "new_idx = [month.upper() for month in sales.index]\n",
    "\n",
    "# Assign new_idx to sales.index\n",
    "sales.index = new_idx\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "\n",
    "# Assign the string 'MONTHS' to sales.index.name\n",
    "sales.index.name = 'MONTHS'\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Assign the string 'PRODUCTS' to sales.columns.name \n",
    "sales.columns.name = 'PRODUCTS'\n",
    "\n",
    "# Print the sales dataframe again\n",
    "print(sales)\n",
    "\n",
    "# You can also build the DataFrame and index independently, and then put them together. \n",
    "# If you take this route, be careful, \n",
    "# as any mistakes in generating the DataFrame or the index can cause the data and the index to be aligned incorrectly.\n",
    "# Generate the list of months: months\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "\n",
    "# Assign months to sales.index\n",
    "sales.index = months\n",
    "\n",
    "# Print the modified sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Extracting elements from the outermost level of a MultiIndex is just like in the case of a single-level Index\n",
    "# Print sales.loc[['CA', 'TX']]\n",
    "print(sales.loc[['CA', 'TX']])\n",
    "\n",
    "# Print sales['CA':'TX']\n",
    "# this selects rows with slicing. different from choosing columns with bracket\n",
    "print(sales['CA':'TX'])\n",
    "\n",
    "# With a MultiIndex, you should always ensure the index is sorted. \n",
    "# You can skip this only if you know the data is already sorted on the index fields.\n",
    "# Set the index to be the columns ['state', 'month']: sales\n",
    "sales = sales.set_index(['state', 'month'])\n",
    "\n",
    "# Sort the MultiIndex: sales\n",
    "sales = sales.sort_index()\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Set the index to the column 'state': sales\n",
    "sales = sales.set_index(['state'])\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Access the data from 'NY'\n",
    "print(sales.loc['NY'])\n",
    "\n",
    "\n",
    "# Looking up indexed data is fast and efficient. And you have already seen that lookups based on the outermost \n",
    "# level of a MultiIndex work just like lookups on DataFrames that have a single-level Index.\n",
    "# Looking up data based on inner levels of a MultiIndex can be a bit trickier.\n",
    "# you need to use slice(None) in the slicing parameter for the outermost dimension(s) instead of the usual :, \n",
    "# or use pd.IndexSlice. \n",
    "# http://pandas.pydata.org/pandas-docs/stable/advanced.html\n",
    "# Look up data for NY in month 1: NY_month1\n",
    "NY_month1 = sales.loc[('NY', 1)]\n",
    "\n",
    "# Look up data for CA and TX in month 2: CA_TX_month2\n",
    "CA_TX_month2 = sales.loc[(['CA', 'TX'], 2),:]\n",
    "\n",
    "# Look up data for all states in month 2: all_month2\n",
    "all_month2 = sales.loc[(slice(None), 2),:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearranging and reshaping data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot - spread rows into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     city weekday visitors signups\n",
      "0  Austin     Mon      326       3\n",
      "1  Austin     Sun      139       7\n",
      "2  Dallas     Mon      456       5\n",
      "3  Dallas     Sun      237      12\n",
      "city    Austin Dallas\n",
      "weekday              \n",
      "Mon        326    456\n",
      "Sun        139    237\n",
      "city    Austin Dallas\n",
      "weekday              \n",
      "Mon          3      5\n",
      "Sun          7     12\n",
      "        visitors        signups       \n",
      "city      Austin Dallas  Austin Dallas\n",
      "weekday                               \n",
      "Mon          326    456       3      5\n",
      "Sun          139    237       7     12\n"
     ]
    }
   ],
   "source": [
    "# In [2]: users\n",
    "# Out[2]: \n",
    "#   weekday    city  visitors  signups\n",
    "# 0     Sun  Austin       139        7\n",
    "# 1     Sun  Dallas       237       12\n",
    "# 2     Mon  Austin       326        3\n",
    "# 3     Mon  Dallas       456        5\n",
    "\n",
    "#prepare the sales dataframe\n",
    "import numpy as np\n",
    "users_values = np.array([['Austin', 'Mon', 326, 3],\n",
    "       ['Austin', 'Sun', 139, 7],\n",
    "       ['Dallas', 'Mon', 456, 5],\n",
    "       ['Dallas', 'Sun', 237, 12]], dtype=object)\n",
    "users = pd.DataFrame(sales_values)\n",
    "users.columns = ['city', 'weekday', 'visitors', 'signups']\n",
    "print users\n",
    "\n",
    "# Pivot the users DataFrame: visitors_pivot\n",
    "visitors_pivot = users.pivot(index='weekday', columns='city', values='visitors')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(visitors_pivot)\n",
    "\n",
    "# Pivot users with signups indexed by weekday and city: signups_pivot\n",
    "signups_pivot = users.pivot(index='weekday', columns='city', values='signups')\n",
    "\n",
    "# Print signups_pivot\n",
    "print(signups_pivot)\n",
    "\n",
    "# Pivot users pivoted by both signups and visitors: pivot\n",
    "pivot = users.pivot(index='weekday', columns='city')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack & Unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        visitors      signups    \n",
      "weekday      Mon  Sun     Mon Sun\n",
      "city                             \n",
      "Austin       326  139       3   7\n",
      "Dallas       456  237       5  12\n",
      "               visitors signups\n",
      "city   weekday                 \n",
      "Austin Mon          326       3\n",
      "       Sun          139       7\n",
      "Dallas Mon          456       5\n",
      "       Sun          237      12\n",
      "        visitors        signups       \n",
      "city      Austin Dallas  Austin Dallas\n",
      "weekday                               \n",
      "Mon          326    456       3      5\n",
      "Sun          139    237       7     12\n",
      "               visitors signups\n",
      "weekday city                   \n",
      "Mon     Austin      326       3\n",
      "        Dallas      456       5\n",
      "Sun     Austin      139       7\n",
      "        Dallas      237      12\n"
     ]
    }
   ],
   "source": [
    "# for multilevel index, the above pivot method won't work\n",
    "# so introduce stack and unstack methods.\n",
    "users = users.set_index(['city', 'weekday'])\n",
    "# Unstack users by 'weekday': byweekday\n",
    "byweekday = users.unstack(level='weekday')\n",
    "\n",
    "# Print the byweekday DataFrame\n",
    "print(byweekday)\n",
    "\n",
    "# Stack byweekday by 'weekday' and print it\n",
    "print(byweekday.stack(level='weekday'))\n",
    "\n",
    "# Unstack users by 'city': bycity\n",
    "bycity = users.unstack(level='city')\n",
    "\n",
    "# Print the bycity DataFrame\n",
    "print(bycity)\n",
    "\n",
    "# Stack bycity by 'city' and print it\n",
    "print(bycity.stack(level='city'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(               visitors signups\n",
       " city   weekday                 \n",
       " Austin Mon          326       3\n",
       "        Sun          139       7\n",
       " Dallas Mon          456       5\n",
       "        Sun          237      12,      city weekday visitors signups\n",
       " 0  Austin     Mon      326       3\n",
       " 1  Austin     Sun      139       7\n",
       " 2  Dallas     Mon      456       5\n",
       " 3  Dallas     Sun      237      12)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack 'city' back into the index of bycity: newusers\n",
    "newusers = bycity.stack(level='city')\n",
    "\n",
    "# Swap the levels of the index of newusers: newusers\n",
    "newusers = newusers.swaplevel(0,1)\n",
    "\n",
    "# Print newusers and verify that the index is not sorted\n",
    "print(newusers)\n",
    "\n",
    "# Sort the index of newusers: newusers\n",
    "newusers = newusers.sort_index()\n",
    "\n",
    "# Print newusers and verify that the index is now sorted\n",
    "print(newusers)\n",
    "\n",
    "# Verify that the new DataFrame is equal to the original\n",
    "print(newusers.equals(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt - \"unpivot\" - Gather columns into rows\n",
    "The goal of melting is to restore a pivoted DataFrame to its original form, or to change it from a wide shape to a long shape. \n",
    "You can explicitly specify the columns that should remain in the reshaped DataFrame with id_vars, and list which columns to convert into values with value_vars.\n",
    "If you don't pass a name to the values in pd.melt(), you will lose the name of your variable. You can fix this by using the value_name keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In [1]: visitors_by_city_weekday\n",
    "# Out[1]: \n",
    "# city     Austin  Dallas\n",
    "# weekday                \n",
    "# Mon         326     456\n",
    "# Sun         139     237\n",
    "\n",
    "# Reset the index: visitors_by_city_weekday\n",
    "visitors_by_city_weekday = visitors_by_city_weekday.reset_index() \n",
    "\n",
    "# Print visitors_by_city_weekday\n",
    "print(visitors_by_city_weekday)\n",
    "\n",
    "# Melt visitors_by_city_weekday: visitors\n",
    "visitors = pd.melt(visitors_by_city_weekday, id_vars=['weekday'], value_name='visitors')\n",
    "\n",
    "# Print visitors\n",
    "print(visitors)\n",
    "\n",
    "# You can move multiple columns into a single column (making the data long and skinny) by \"melting\" multiple columns. \n",
    "# In [3]: users\n",
    "# Out[3]: \n",
    "#   weekday    city  visitors  signups\n",
    "# 0     Sun  Austin       139        7\n",
    "# 1     Sun  Dallas       237       12\n",
    "# 2     Mon  Austin       326        3\n",
    "# 3     Mon  Dallas       456        5\n",
    "# Melt users: skinny\n",
    "skinny = pd.melt(users, id_vars=['city', 'weekday'])\n",
    "\n",
    "# Print skinny\n",
    "print(skinny)\n",
    "\n",
    "\n",
    "\n",
    "# # Obtaining key-value pairs with melt()\n",
    "# Sometimes, all you need is some key-value pairs, and the context does not matter. \n",
    "# If said context is in the index, you can easily obtain what you want.\n",
    "\n",
    "# Set the new index: users_idx\n",
    "users_idx = users.set_index(['city', 'weekday'])\n",
    "\n",
    "# Print the users_idx DataFrame\n",
    "print(users_idx)\n",
    "\n",
    "# Obtain the key-value pairs: kv_pairs\n",
    "kv_pairs = pd.melt(users_idx, col_level=0)\n",
    "\n",
    "# Print the key-value pairs\n",
    "print(kv_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame with the appropriate pivot table: by_city_day\n",
    "by_city_day = users.pivot_table(index='weekday', columns='city')\n",
    "\n",
    "# Print by_city_day\n",
    "print(by_city_day)\n",
    "\n",
    "\n",
    "\n",
    "# Use a pivot table to display the count of each column: count_by_weekday1\n",
    "count_by_weekday1 = users.pivot_table(index='weekday', aggfunc='count')\n",
    "\n",
    "# Print count_by_weekday\n",
    "print(count_by_weekday1)\n",
    "\n",
    "# Replace 'aggfunc='count'' with 'aggfunc=len': count_by_weekday2\n",
    "count_by_weekday2 = users.pivot_table(index='weekday', aggfunc=len)\n",
    "\n",
    "# Verify that the same result is obtained\n",
    "print('==========================================')\n",
    "print(count_by_weekday1.equals(count_by_weekday2))\n",
    "\n",
    "\n",
    "# Create the DataFrame with the appropriate pivot table: signups_and_visitors\n",
    "signups_and_visitors = users.pivot_table(index='weekday', aggfunc=sum)\n",
    "\n",
    "# Print signups_and_visitors\n",
    "print(signups_and_visitors)\n",
    "\n",
    "# Add in the margins: signups_and_visitors_total \n",
    "signups_and_visitors_total = users.pivot_table(index='weekday', aggfunc=sum, margins=True)\n",
    "\n",
    "# Print signups_and_visitors_total\n",
    "print(signups_and_visitors_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data \n",
    "the main advantages of storing data explicitly as categorical types instead of object types:\n",
    "1. computations are faster\n",
    "2. Categorical data require less space in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group titanic by 'pclass'\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Aggregate 'survived' column of by_class by count\n",
    "count_by_class = by_class['survived'].count()\n",
    "\n",
    "# Print count_by_class\n",
    "print(count_by_class)\n",
    "\n",
    "# Group titanic by 'embarked' and 'pclass'\n",
    "by_mult = titanic.groupby(['embarked', 'pclass'])\n",
    "\n",
    "# Aggregate 'survived' column of by_mult by count\n",
    "count_mult = by_mult['survived'].count()\n",
    "\n",
    "# Print count_mult\n",
    "print(count_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region\n",
      "America                       74.037350\n",
      "East Asia & Pacific           73.405750\n",
      "Europe & Central Asia         75.656387\n",
      "Middle East & North Africa    72.805333\n",
      "South Asia                    68.189750\n",
      "Sub-Saharan Africa            57.575080\n",
      "Name: 2010, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# https://s3.amazonaws.com/assets.datacamp.com/production/course_1650/datasets/life_expectancy.csv\n",
    "life_fname = 'life_expectancy.csv'\n",
    "regions_fname = 'regions.csv'\n",
    "# Read life_fname into a DataFrame: life\n",
    "life = pd.read_csv(life_fname, index_col='Country')\n",
    "\n",
    "# Read regions_fname into a DataFrame: regions\n",
    "regions = pd.read_csv(regions_fname, index_col='Country')\n",
    "\n",
    "# Group life by regions['region']: life_by_region\n",
    "# By setting the index of both DataFrames to the country name, \n",
    "# you'll then use the region information to group the countries in \n",
    "# the life expectancy DataFrame and compute the mean value for 2010.\n",
    "# life's index groupby regions['region'] needs they share the same index and use regions['region'] as a mapping\n",
    "life_by_region = life.groupby(regions['region'])\n",
    "# doesn't works if remove the country from the index\n",
    "# life.reset_index().groupby(regions['region'])['2010'].mean()\n",
    "# life.groupby(regions.reset_index()['region'])\n",
    "\n",
    "# Print the mean over the '2010' column of life_by_region\n",
    "print(life_by_region['2010'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c27282c864a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Group titanic by 'pclass': by_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mby_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pclass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Select 'age' and 'fare'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic' is not defined"
     ]
    }
   ],
   "source": [
    "# The .agg() method can be used with a tuple or list of aggregations as input.\n",
    "# When applying multiple aggregations on multiple columns, the aggregated DataFrame has a multi-level column index.\n",
    "\n",
    "# Group titanic by 'pclass': by_class\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Select 'age' and 'fare'\n",
    "by_class_sub = by_class[['age','fare']]\n",
    "\n",
    "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
    "aggregated = by_class_sub.agg(['max', 'median'])\n",
    "\n",
    "# Print the maximum age in each class\n",
    "print(aggregated.loc[:, ('age','max')])\n",
    "\n",
    "# Print the median fare in each class\n",
    "print(aggregated.loc[:, ('fare', 'median')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   population  child_mortality       gdp\n",
      "Year region                                                             \n",
      "1973 America                     5.456463e+08        88.629667   24363.0\n",
      "     East Asia & Pacific         1.384693e+09       100.646087   86215.0\n",
      "     Europe & Central Asia       7.554238e+08        46.040000   36609.0\n",
      "     Middle East & North Africa  1.514955e+08       121.090000  163972.0\n",
      "     South Asia                  7.664669e+08       210.837500    1068.0\n",
      "     Sub-Saharan Africa          1.483449e+08       219.697143    7733.0\n"
     ]
    }
   ],
   "source": [
    "# If you have a DataFrame with a multi-level row index, the individual levels can be used to perform the groupby. \n",
    "\n",
    "#                                   fertility    life  population  \\\n",
    "# Year region  Country                                              \n",
    "# 1964 America Antigua and Barbuda      4.250  63.775     58653.0   \n",
    "#              Argentina                3.068  65.388  21966478.0   \n",
    "#              Aruba                    4.059  67.113     57031.0   \n",
    "#              Bahamas                  4.220  64.189    133709.0   \n",
    "#              Barbados                 4.094  62.819    234455.0   \n",
    "\n",
    "#                                   child_mortality      gdp  \n",
    "# Year region  Country                                        \n",
    "# 1964 America Antigua and Barbuda            72.78   5008.0  \n",
    "#              Argentina                      57.43   8227.0  \n",
    "#              Aruba                            NaN   5505.0  \n",
    "#              Bahamas                        48.56  18160.0  \n",
    "#              Barbados                       64.70   5681.0\n",
    "                \n",
    "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
    "gapminder = pd.read_csv('gapminder.csv', index_col=['Year', 'region', 'Country']).sort_index()\n",
    "\n",
    "# Group gapminder by 'Year' and 'region: by_year_region\n",
    "# groupby again on the index without duplicated index\n",
    "by_year_region = gapminder.groupby(level=['Year', 'region'])\n",
    "\n",
    "# Define the function to compute spread: spread\n",
    "def spread(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "# Create the dictionary: aggregator\n",
    "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
    "\n",
    "# Aggregate by_year_region using the dictionary: aggregated\n",
    "aggregated = by_year_region.agg(aggregator)\n",
    "\n",
    "# Print the last 6 entries of aggregated \n",
    "print(aggregated.tail(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon    48\n",
      "Sat     7\n",
      "Thu    59\n",
      "Tue    13\n",
      "Wed    48\n",
      "Name: Units, dtype: int64\n",
      "{'Mon': Int64Index([0, 1, 8, 9, 12], dtype='int64'), 'Sat': Int64Index([7, 15, 16], dtype='int64'), 'Thu': Int64Index([5, 6, 13, 14, 18], dtype='int64'), 'Tue': Int64Index([2], dtype='int64'), 'Wed': Int64Index([3, 4, 10, 11, 17], dtype='int64')}\n",
      "{'Mon': Int64Index([4, 5, 2, 17, 14], dtype='int64'), 'Sat': Int64Index([12, 13, 8], dtype='int64'), 'Thu': Int64Index([6, 11, 9, 1, 18], dtype='int64'), 'Tue': Int64Index([7], dtype='int64'), 'Wed': Int64Index([0, 16, 10, 15, 3], dtype='int64')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/airings/anaconda2/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# the function passed to groupby is applied on the each of index values\n",
    "\n",
    "# Groubpy operations can also be performed on transformations of the index values. \n",
    "# In the case of a DateTimeIndex, we can extract portions of the datetime over which to group.\n",
    "\n",
    "# Read file: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Create a groupby object: by_day\n",
    "# use .strftime('%a') to transform the index datetime values to abbreviated days of the week.\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\n",
    "# Create sum: units_sum\n",
    "units_sum = by_day['Units'].sum()\n",
    "\n",
    "# Print units_sum\n",
    "print(units_sum)\n",
    "\n",
    "# Trying following to find out, the results are different.\n",
    "print(sales.reset_index().sort_index().groupby(sales.index.strftime('%a')).groups)\n",
    "print(sales.reset_index().sort('Company').groupby(sales.index.strftime('%a')).groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            region  fertility    life  population  \\\n",
      "Country                                                             \n",
      "Guatemala                  America      3.974  71.100  14388929.0   \n",
      "Haiti                      America      3.350  45.000   9993247.0   \n",
      "Tajikistan   Europe & Central Asia      3.780  66.830   6878637.0   \n",
      "Timor-Leste    East Asia & Pacific      6.237  65.952   1124355.0   \n",
      "\n",
      "             child_mortality     gdp  \n",
      "Country                               \n",
      "Guatemala               34.5  6849.0  \n",
      "Haiti                  208.8  1518.0  \n",
      "Tajikistan              52.6  2110.0  \n",
      "Timor-Leste             63.8  1777.0  \n"
     ]
    }
   ],
   "source": [
    "# you can apply a .transform() method after grouping to apply a function to groups of data independently. \n",
    "# The z-score is also useful to find outliers: a z-score value of +/- 3 is generally considered to be an outlier.\n",
    "\n",
    "# Import zscore\n",
    "from scipy.stats import zscore\n",
    "\n",
    "gapminder_2010 = pd.read_csv('gapminder_2010.csv', index_col=['Country']).sort_index()\n",
    "# Group gapminder_2010: standardized\n",
    "standardized = gapminder_2010.groupby('region')['life', 'fertility'].transform(zscore)\n",
    "\n",
    "# Construct a Boolean Series to identify outliers: outliers\n",
    "outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)\n",
    "\n",
    "# Filter gapminder_2010 by the outliers: gm_outliers\n",
    "gm_outliers = gapminder_2010.loc[outliers]\n",
    "\n",
    "# Print gm_outliers\n",
    "print(gm_outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing data (imputation) by group\n",
    "Many statistical and machine learning packages cannot determine the best action to take when missing data entries are encountered. Dealing with missing data is natural in pandas (both in using the default behavior and in defining a custom behavior). In Chapter 1, you practiced using the .dropna() method to drop missing values. Now, you will practice imputing missing values. You can use .groupby() and .transform() to fill missing data appropriately for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  pclass  survived                                     name  \\\n",
      "1299        1299       3         0                      Yasbeck, Mr. Antoni   \n",
      "1300        1300       3         1  Yasbeck, Mrs. Antoni (Selini Alexander)   \n",
      "1301        1301       3         0                     Youseff, Mr. Gerious   \n",
      "1302        1302       3         0                        Yousif, Mr. Wazli   \n",
      "1303        1303       3         0                    Yousseff, Mr. Gerious   \n",
      "1304        1304       3         0                     Zabour, Miss. Hileni   \n",
      "1305        1305       3         0                    Zabour, Miss. Thamine   \n",
      "1306        1306       3         0                Zakarian, Mr. Mapriededer   \n",
      "1307        1307       3         0                      Zakarian, Mr. Ortin   \n",
      "1308        1308       3         0                       Zimmerman, Mr. Leo   \n",
      "\n",
      "         sex   age  sibsp  parch  ticket     fare cabin embarked boat   body  \\\n",
      "1299    male  27.0      1      0    2659  14.4542   NaN        C    C    NaN   \n",
      "1300  female  15.0      1      0    2659  14.4542   NaN        C  NaN    NaN   \n",
      "1301    male  45.5      0      0    2628   7.2250   NaN        C  NaN  312.0   \n",
      "1302    male  25.0      0      0    2647   7.2250   NaN        C  NaN    NaN   \n",
      "1303    male  25.0      0      0    2627  14.4583   NaN        C  NaN    NaN   \n",
      "1304  female  14.5      1      0    2665  14.4542   NaN        C  NaN  328.0   \n",
      "1305  female  22.0      1      0    2665  14.4542   NaN        C  NaN    NaN   \n",
      "1306    male  26.5      0      0    2656   7.2250   NaN        C  NaN  304.0   \n",
      "1307    male  27.0      0      0    2670   7.2250   NaN        C  NaN    NaN   \n",
      "1308    male  29.0      0      0  315082   7.8750   NaN        S  NaN    NaN   \n",
      "\n",
      "     home.dest  \n",
      "1299       NaN  \n",
      "1300       NaN  \n",
      "1301       NaN  \n",
      "1302       NaN  \n",
      "1303       NaN  \n",
      "1304       NaN  \n",
      "1305       NaN  \n",
      "1306       NaN  \n",
      "1307       NaN  \n",
      "1308       NaN  \n"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "# Create a groupby object: by_sex_class\n",
    "by_sex_class = titanic.groupby(['sex','pclass'])\n",
    "\n",
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "# Impute age and assign to titanic['age']\n",
    "titanic['age'] = by_sex_class['age'].transform(impute_median)\n",
    "\n",
    "# Print the output of titanic.tail(10)\n",
    "print(titanic.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other transformations with .apply\n",
    "The .apply() method when used on a groupby object performs an arbitrary function on each of the groups. These functions can be aggregations, transformations or more complex workflows. The .apply() method will then combine the results in an intelligent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                regional spread(gdp)    z(gdp)\n",
      "Country                                       \n",
      "United States                47855.0  3.013374\n",
      "United Kingdom               89037.0  0.572873\n",
      "China                        96993.0 -0.432756\n"
     ]
    }
   ],
   "source": [
    "def disparity(gr):\n",
    "    # Compute the spread of gr['gdp']: s\n",
    "    s = gr['gdp'].max() - gr['gdp'].min()\n",
    "    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z\n",
    "    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()\n",
    "    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}\n",
    "    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})\n",
    "\n",
    "# Group gapminder_2010 by 'region': regional\n",
    "regional = gapminder_2010.groupby('region')\n",
    "\n",
    "# Apply the disparity function on regional: reg_disp\n",
    "reg_disp = regional.apply(disparity)\n",
    "\n",
    "# Print the disparity of 'United States', 'United Kingdom', and 'China'\n",
    "print(reg_disp.loc[['United States', 'United Kingdom', 'China']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and filtering with .apply()\n",
    "By using .apply(), you can write functions that filter rows within groups. The .apply() method will handle the iteration over individual groups and then re-combine them back into a Series or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "female    0.913043\n",
      "male      0.312500\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def c_deck_survival(gr):\n",
    "\n",
    "    c_passengers = gr['cabin'].str.startswith('C').fillna(False)\n",
    "\n",
    "    return gr.loc[c_passengers, 'survived'].mean()\n",
    "\n",
    "# Create a groupby object using titanic over the 'sex' column: by_sex\n",
    "by_sex = titanic.groupby('sex')\n",
    "\n",
    "# Call by_sex.apply with the function c_deck_survival and print the result\n",
    "c_surv_by_sex = by_sex.apply(c_deck_survival)\n",
    "\n",
    "# Print the survival rates\n",
    "print(c_surv_by_sex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and filtering with .filter()\n",
    "You can use groupby with the .filter() method to remove whole groups of rows from a DataFrame based a boolean condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company\n",
      "Acme Coporation    34\n",
      "Hooli              30\n",
      "Initech            30\n",
      "Mediacore          45\n",
      "Streeplex          36\n",
      "Name: Units, dtype: int64\n",
      "                       Company   Product  Units\n",
      "Date                                           \n",
      "2015-02-02 21:00:00  Mediacore  Hardware      9\n",
      "2015-02-04 15:30:00  Streeplex  Software     13\n",
      "2015-02-09 09:00:00  Streeplex   Service     19\n",
      "2015-02-09 13:00:00  Mediacore  Software      7\n",
      "2015-02-19 11:00:00  Mediacore  Hardware     16\n",
      "2015-02-19 16:00:00  Mediacore   Service     10\n",
      "2015-02-21 05:00:00  Mediacore  Software      3\n",
      "2015-02-26 09:00:00  Streeplex   Service      4\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Group sales by 'Company': by_company\n",
    "by_company = sales.groupby('Company')\n",
    "\n",
    "# Compute the sum of the 'Units' of by_company: by_com_sum\n",
    "by_com_sum = by_company['Units'].sum()\n",
    "print(by_com_sum)\n",
    "\n",
    "# Filter 'Units' where the sum is > 35: by_com_filt\n",
    "by_com_filt = by_company.filter(lambda g: g['Units'].sum() > 35)\n",
    "print(by_com_filt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and grouping with .map()\n",
    "You have seen how to group by a column, or by multiple columns. Sometimes, you may instead want to group by a function/transformation of a column. The key here is that the Series is indexed the same way as the DataFrame. You can also mix and match column grouping with Series grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "over 10     0.366748\n",
      "under 10    0.609756\n",
      "Name: survived, dtype: float64\n",
      "age       pclass\n",
      "over 10   1         0.617555\n",
      "          2         0.380392\n",
      "          3         0.238897\n",
      "under 10  1         0.750000\n",
      "          2         1.000000\n",
      "          3         0.446429\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the Boolean Series: under10\n",
    "under10 = (titanic['age'] < 10).map({True:'under 10', False:'over 10'})\n",
    "\n",
    "# Group by under10 and compute the survival rate\n",
    "survived_mean_1 = titanic.groupby(under10)['survived'].mean()\n",
    "print(survived_mean_1)\n",
    "\n",
    "# Group by under10 and pclass and compute the survival rate\n",
    "survived_mean_2 = titanic.groupby([under10, 'pclass'])['survived'].mean()\n",
    "print(survived_mean_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together \n",
    "Case Study - Summer Olympics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read file: sales\n",
    "medals = pd.read_csv('ALL_MEDALISTS.csv', header=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Edition</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Discipline</th>\n",
       "      <th>Athlete</th>\n",
       "      <th>NOC</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Event</th>\n",
       "      <th>Event_gender</th>\n",
       "      <th>Medal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>1896</td>\n",
       "      <td>Aquatics</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>HAJOS, Alfred</td>\n",
       "      <td>HUN</td>\n",
       "      <td>Men</td>\n",
       "      <td>100m freestyle</td>\n",
       "      <td>M</td>\n",
       "      <td>Gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>1896</td>\n",
       "      <td>Aquatics</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>HERSCHMANN, Otto</td>\n",
       "      <td>AUT</td>\n",
       "      <td>Men</td>\n",
       "      <td>100m freestyle</td>\n",
       "      <td>M</td>\n",
       "      <td>Silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>1896</td>\n",
       "      <td>Aquatics</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>DRIVAS, Dimitrios</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Men</td>\n",
       "      <td>100m freestyle for sailors</td>\n",
       "      <td>M</td>\n",
       "      <td>Bronze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>1896</td>\n",
       "      <td>Aquatics</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>MALOKINIS, Ioannis</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Men</td>\n",
       "      <td>100m freestyle for sailors</td>\n",
       "      <td>M</td>\n",
       "      <td>Gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>1896</td>\n",
       "      <td>Aquatics</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>CHASAPIS, Spiridon</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Men</td>\n",
       "      <td>100m freestyle for sailors</td>\n",
       "      <td>M</td>\n",
       "      <td>Silver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     City  Edition     Sport Discipline             Athlete  NOC Gender  \\\n",
       "0  Athens     1896  Aquatics   Swimming       HAJOS, Alfred  HUN    Men   \n",
       "1  Athens     1896  Aquatics   Swimming    HERSCHMANN, Otto  AUT    Men   \n",
       "2  Athens     1896  Aquatics   Swimming   DRIVAS, Dimitrios  GRE    Men   \n",
       "3  Athens     1896  Aquatics   Swimming  MALOKINIS, Ioannis  GRE    Men   \n",
       "4  Athens     1896  Aquatics   Swimming  CHASAPIS, Spiridon  GRE    Men   \n",
       "\n",
       "                        Event Event_gender   Medal  \n",
       "0              100m freestyle            M    Gold  \n",
       "1              100m freestyle            M  Silver  \n",
       "2  100m freestyle for sailors            M  Bronze  \n",
       "3  100m freestyle for sailors            M    Gold  \n",
       "4  100m freestyle for sailors            M  Silver  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edition\n",
       "1896     20\n",
       "1900     55\n",
       "1904    394\n",
       "1908     63\n",
       "1912    101\n",
       "Name: Medal, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose you have loaded the data into a DataFrame medals. \n",
    "# You now want to find the total number of medals awarded to the USA per edition. \n",
    "USA_edition_grouped = medals.loc[medals.NOC == 'USA'].groupby('Edition')\n",
    "USA_edition_grouped['Medal'].count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the 'NOC' column of medals: country_names\n",
    "country_names = medals['NOC']\n",
    "\n",
    "# Count the number of medals won by each country: medal_counts\n",
    "medal_counts1 = medals.groupby(country_names)['Medal'].count().sort_values(ascending=False)\n",
    "medal_counts2 = country_names.value_counts()\n",
    "# medal_counts2.index.name = 'NOC'\n",
    "# medal_counts1.index, medal_counts2.index,\n",
    "# len(medal_counts1) == len(medal_counts2)\n",
    "re = medal_counts1.sort_index() == medal_counts2.sort_index()\n",
    "re[re] \n",
    "re.sum()\n",
    "# Print top 15 countries ranked by medals\n",
    "# medal_counts2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
